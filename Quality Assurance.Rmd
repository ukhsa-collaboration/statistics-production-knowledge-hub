---
title: "Quality Assurance"
author: "Olivia Swarthout"
date: "24 October, 2022"    #"`r format(Sys.time(), '%d %B, %Y')`"
draft: true
output: 
  html_document:
    toc: true
    toc_depth: 3
    number_sections: false
    template: "- assets/Templates/html_template.html"
  word_document:
    reference_docx: "- assets/Templates/UKHSA plain document template.dotx"
knit: (function(inputFile, encoding){rmarkdown::render(inputFile, envir = globalenv(), encoding = encoding, output_dir = "./", output_format = "all")})

versioncontrol: "`r ukhsaGuidanceDocs::VersionHistory(  c('24/10/2022', 'First draft')  )`"
---


```{r setup, echo=F, include=F}
library(tidyverse)
library(readxl)
library(ukhsaGuidanceDocs)
knitr::opts_chunk$set(echo = F)
SetUpReferenceList("- assets/QPD knowledge bank.xlsx")
```


## Coverage

This guidance applies to everyone working in the production of statistics or other numerical analysis, whether for internal or external use.

## Summary of recommendations

1. Quality assurance should be considered at every step of analysis by everyone involved.
2. Quality assurance should ideally be planned in advance of statistical analysis, with roles, responsibilities, and the extent of assurance activities understood fully by the entire team. 
3. Quality analysis should be appropriate to the scope, risks, methodology, and data source of a piece of analysis. 
4. Analytical and quality assurance processes should be logged in detail as they are carried out. A designated assurer should confirm that a sufficient degree and standard of assurance has been conducted.
5. The quality assurance process and any outstanding quality concerns should be communicated alongside the final results of analysis in order to quantify confidence in the results.

## Introduction

### Defining quality

At the most basic level, the quality of a statistical output may be thought of as its "fitness for purpose" `r addRef(1,44,37,34)`. The ONS Code of Practice uses the five Dimensions of Quality of the European Statistical System (ESS) Code of Practice as criteria for assessing fitness for purpose of statistical outputs. These five dimensions are:

* Relevance: statistics meet the needs of users
* Accuracy and Reliability: statistics accurately and reliably portray reality
* Timeliness and Punctuality: statistics are released in a timely and punctual manner
* Comparability and Coherence: statistics are consistent internally, over time and 
comparable between regions and countries; it is possible to combine 
and make joint use of related data from different data sources
* Accessibility and Clarity: statistics are presented in a clear and understandable 
form, released in a suitable and convenient manner, available and 
accessible on an impartial basis with supporting metadata and 
guidance

Quality assurance (QA) of a statistical output is the process of ensuring that these criteria have been met during its production and proving this fact to users.

### Why we need quality assurance

The ONS Code of Practice includes quality as one of the three core pillars that support public confidence in statistics `r addRef(1)`. Under Q3 of the Quality Pillar, the Code of Practice requires all producers of statistics and other analytical outputs to explain how they have assured that their work is accurate, reliable, coherent and timely `r addRef(1)`. Similarly, a key goal of the Government Statistical Service (GSS) Quality Strategy is to ensure our outputs are of sufficient quality and communicate the quality implications to users `r addRef(84)`. 

Thus, we see that there is a requirement not only to guarantee quality but to demonstrate it to users, allowing them to use statistical publications with complete confidence in their validity `r addRef(84,85)`. However, ensuring high quality is also key to building trust among those who are involved in policy and decision making `r addRef(44,37)`. Outputs that attain high levels of fitness-for-purpose allow better decisions to be made with greater efficiency, supporting early trend identification and risk mitigation in the public health sector`r addRef(37, 84,32)`. The Goldacre Review explains the very material benefits of high-quality analysis, noting that “these kinds of analyses deliver direct improvements in patient care by identifying problems early, and improving the efficiency of services for all.” `r addRef(84)` The risks posed by poor quality analysis are just as immediate and can include financial and legal damage, as well as, in extreme cases, risks to health and livelihood `r addRef(34)`. It is therefore crucial that internal and external outputs are subject to rigorous quality assurance measures. 

## Roles and responsibilities

Goal 1 of the GSS Quality Strategy sets out the need for everyone involved in statistical production to understand the nature and importance of their role in assuring quality `r addRef(84)`. Team members should understand the exact nature and extent of QA that is planned for a specific project as well as their expected contributions `r addRef(85)`. 

The Aqua Book defines three key roles for QA of statistical analysis: the commissioner, the analyst, and the assurer `r addRef(34)`. Other government publications such as Government Functional Standard 010: Analysis use similar roles to demonstrate the delegation of responsibility for assurance of analysis `r addRef(32)`. While in practice, the exact nature of roles required may differ based on project scope and requirements, these provide a useful framework for considering the different ways QA should be incorporated into the life cycle of a project. 
 
The roles can be summarised as follows:

* **The person commissioning analysis** (“the commissioner”) is responsible for developing the question of interest and communicating this to the analyst and assurer so that they are able to identify the most appropriate methods for analysis and assurance. They should clearly express the needs of the project and risks that are entailed so that proportionate levels of QA can be decided upon by the analyst. People of all levels of seniority commission analysis, and the commissioner may be acting on behalf of a government decision-maker or other customer. 
* **The person responsible for analysis** (“the analyst”)  must work with the commissioner to help develop the question of interest in a manner that allows it to be answered appropriately as meets the needs of the commissioner. They are responsible for planning and conducting analysis in a methodologically sound way and planning how they will demonstrate assured quality to the commissioner, assurer, and ultimately the end users of the analytical output. As needed, the analyst may bring in third party analysts to provide specialised advice and independent quality assurance. Final results and records of all activities undertaken will be provided by the analyst, who must communicate to the commissioner the implications of their results as impacted by its strengths, limitations, uncertainty, and context. In practice, there may often be more than one analyst involved in a project: the use of the term “the analyst” throughout this document is used for brevity’s sake rather than to exclude this possibility.
* **The person responsible for analytical assurance** (“the assurer”) must ensure throughout that appropriate quality assurance is taking place. This person is not necessarily directly involved in conducting QA and does not need to be an analyst themselves, although QA may benefit from an assurer who understands the analytical complexities of the project in order to identify issues with the output. However, the assurer should not be one of the analysts involved in conducting the actual analysis in order to maintain independence. As the assurer must sign off on the QA activities, they should be of sufficient seniority to take responsibility for the output and advise the commissioner on whether quality concerns have been sufficiently addressed and the implications of any remaining risks: typically, they would be a senior analyst or analytical project manager.

More information about what each of these roles entails at each step of a project can be found in the next section. However, it is important to emphasise that QA does not begin and end with designated roles and project components. Quality can be best guaranteed by a culture of transparency and accountability that promotes continual critical thinking and open communication by all members of a team. This includes encouraging everyone on the team to be open about their mistakes and concerns without fear of repercussions or dismissive responses from senior staff. 

## How is Quality Assurance Conducted?

At each stage of analysis, documentation should be produced detailing QA measures that have been undertaken. This helps guarantee that analysis and QA are transparent and reproducible, in line with ONS Code of Practice Q3.3 `r addRef(1)`, thereby helping to create high-quality statistics that retain value over time. 

This section contains examples of questions that can guide QA throughout a project and suggested items of documentation that can be produced. However, the exact scale and scope of these QA activities should be considered on a case-by-case basis, ensuring that they are appropriate and proportionate to the needs of analysis `r addRef(34)`. More rigorous QA will demand more time and resources, and so the extent of QA should be justified by the level of risk present `r addRef(34,85)`. 

### Ensuring data quality

Data may come from any of a large number of sources and may even be gathered by the analysts themselves. The complexities involved in gathering, storing, managing, and accessing data introduce many potential sources of error for a piece of analysis - before analysis even commences! 

Ensuring high quality as the point of data collection is an extremely involved undertaking that depends on a huge number of situational factors and is therefore beyond the scope of this guidance. However, the validation of subsequent stages of the "data lifecycle" bears discussing here as it is a key aspect of ensuring high quality statistical outputs. Under Q3 of the Quality Pillar, the ONS Code of Practice explains that "Statistics should be based on the most appropriate data to meet intended uses. The impact of any data limitations for use should be assessed, minimised and explained." `r addRef(1)` 

When obtaining data from a database or other storage platform, it is most preferable for the analysts themselves to access the data themselves rather than request it from a provider.  However, this may not be possible in some circumstances due to restrictions on data access permissions, and data may have to be requested from the manager of a database. In this case, the data providers should keep detailed records of the queries used to obtain the requested data. Our guidance on reproducible analytical pipelines (RAP) notes that software such as SQL can be used to query databases using code, thus providing a record of how data was obtained. A data provider should ideally also supply information about the format and details of each variable as well as the source and methodology of collection.`r addRef(86,1)` 

In line with our RAP guidance, before data cleaning begins, a copy of the original data should be saved by the analyst. This allows for the data cleaning process to be documented from beginning to end. A degree of manual checking is generally necessary in order for analysts to assess the presence of any unreasonable values or distributions `r addRef(86)`, but automated checks can be built into the data cleaning pipeline as well. Tools such as RMarkdown notebooks can be used to generate detailed summaries of data and flag any potential errors. Any potential bias, uncertainty, or missing values in the data should be documented alongside the implications these have for the final outputs. `r addRef(34)`

### Quality assurance responsibility throughout analysis

The responsibilities described below are examples of how leadership on tasks can be delegated between commissioner, analyst, and assurer. No single member of a team should be responsible for a task in its entirety, without any input from colleagues `r addRef(34)`. Though it may be one person’s job to lead and shape outputs for a specific aspect of analysis, all team members should have a thorough understanding of every stage that allows them to contribute their skills and knowledge toward the final product. 

### 1. Commissioner engagement

This is usually the most preliminary stage of analysis and focuses on identifying the question that the analysis should address, as well as establishing key context for the analyst’s work. 

#### Suggested documentation:
* Specification or scoping document explaining the intended outcomes of the analysis, which may be edited in subsequent stages as details of the requested work change `r addRef(34)`

#### Responsibilities:
* The **Commissioner** ensures that the intentions and complexities of the problem at hand are clearly communicated and that sufficient resources are secured for the analysis, including for appropriate quality assurance. 
* The	**Analyst** identifies the levels of quality, certainty, and precision that are required for the analysis and communicates with stakeholders to ensure that expectations are aligned with what is possible to deliver
*	The **Assurer** validates the proposed levels of quality and precision and signs off on the scoping document 

#### Guiding questions:
* Have user needs been adequately assessed? `r addRef(34)`.
* Does the planned analysis align with user needs and project priorities? `r addRef(34)`
* Will the proposed time frames allow for adequate quality assurance? `r addRef(34)`
* Have areas of highest risk been identified and addressed? `r addRef(46)`
*	What is our data source? Is data quality-assured and appropriate for our purposes? `r addRef(32)`

### 2. Planning analysis

This stage requires a higher degree of technical input as the commission details are converted into an analysis plan. While a greater degree of responsibility may therefore fall upon the analyst here, it is important for the commissioner to continually confirm that their specified needs will be met by the proposed methodology. 

#### Suggested documentation:
* QA plan that specifies steps that will be taken at each step of the analysis to assure quality.`r addRef(34,46)`.

#### Responsibilities:
* The **Commissioner** maintains contact with the analyst and assurer to continually confirm that time frames will be sufficient for proposed levels of quality assurance
*	The **Analyst** must consider how they will ensure that their work will be robust, accurate, and adequate to address the research question, including considering the limits and uncertainty that these methods will entail and how these will be communicated in the final product. They must also draw up plans for QA of their work and identify suitable independent analysts who can provide QA
* The	**Assurer** checks that the proposed level of QA for the analysis will be sufficient for the needs of the project

#### Guiding questions:
*	Is the planned analytical pipeline sufficiently automated, so as to reduce the risk of human error? `r addRef(46,35)`
*	Where automation is not possible, are risks of manual steps sufficiently mitigated?  `r addRef(46)`
*	Is the planned process transparent and reproducible? `r addRef(32)`
*	Are the methods we are using best suited for the project? `r addRef(85)`
*	Have any potential quality issues in the data been sufficiently considered and addressed?  `r addRef(85)`

### 3. Analysis

It is important that analysis and QA of analysis follow written plans exactly, and that any deviation from these plans is thoroughly documented and communicated to the assurer and commissioner. A complete audit trail will allow for the sources of any issues that are identified during QA to be found more quickly. Ensuring a reproducible process is also a key part of aiding in peer reviews and audits after analysis has been completed.

It may aid efficiency to run QA activities parallel to analysis, passing outputs to assurers individually as they are produced. 

#### Suggested documentation:
*	Technical documentation of methods used `r addRef(34)`
    *	Includes documentation and commenting of code `r addRef(28)`
    *	Records of changes made, ideally managed via version control software such as Git `r addRef(28)`
*	Documentation and justification of assumptions `r addRef(34)`
*	QA log detailing automated and manual validation measures conducted `r addRef(34)`

#### Responsibilities:
* The **Commissioner** ensures that sufficient quality checking of analysis is being conducted for the needs of the project
* The	**Analyst** extracts and manages data, ensuring that data formats, units, and context are understood. They should record details, issues, and assumptions regarding the data and ensure that any instances of missing data are appropriately addressed and the consequences of missing data are recorded. As they follow their analytical plan, they should make note of any changes that they make to the plan as well as all QA measures that have been implemented. Ideally, they should call upon other analysts who are not involved in the project to conduct quality assurance. Where necessary, they may also draw upon the skill of experts who can provide more specialised technical advice
*	The **Assurer** continually confirms that quality standards are being met and that the level of QA that has been carried out remains sufficient for the decision being supported

#### Guiding questions:
*	Are there any anomalies or unexpected trends in the data? Have they been sufficiently investigated? `r addRef(34,85)`
*	When changes have been made to analysis and QA methodologies, are they justified, well-documented, and clearly communicated? `r addRef(28)`
*	What assumptions does our analysis rely upon? Are these justified? `r addRef(86)`
*	Have the established best practice standards for code been followed? `r addRef(86,28)`

### 4. Delivery of analysis

At this stage, it should be confirmed that the results of analysis are presented in a straightforward and transparent manner with all necessary context. The analysis may be approved in stages and may be sent back and forth between the commissioner, analyst, and assurer several times before it can be finalised. 

#### Suggested documentation:
*	QA sign-off by assurer verifying that quality has been assured to the required standard `r addRef(86)`
*	Clarification of data and analysis quality, and the implications on results `r addRef(37)`
*	Quality and methods section of publication `r addRef(47)`

#### Responsibilities:
* The **Commissioner** passes results on to decision-makers and stakeholders, ensuring that the details of results as well as limitations and outstanding risks are clearly communicated so that the analysis can be used appropriately
* The	**Analyst** ensures that results, risks, and limitations are clearly communicated to the commissioner and well-recorded in the final publication. They should reflect on the strong and weak aspects of the analytical process and any errors that were made, considering how they might apply what they have learned to future analysis
*	The **Assurer** confirms that suitable QA has taken place and that an audit trail is in place that clarifies the extent of validation activities. If so, the assurer signs off on the finished product. Otherwise, they should discuss concerns with the analyst and commissioner and ensure these are addressed before authorising delivery

#### Guiding questions:
* Without any input from analytical team, could a third party reproduce our results? `r addRef(86)`
*	If we repeat our processes with different software or methods, will we get the same results? `r addRef(85)`
*	Have strengths and limitations been communicated clearly and effectively to users, stakeholders, and team members? `r addRef(1,35)`
*	Are data visualisations accessible and easy to understand? `r addRef(47)`
* Are sources cited and correctly linked? `r addRef(85,32)`
*	Have technical terms been defined in simple English, if necessary? `r addRef(47)`
*	Is the output accurate and error-free? `r addRef(46)`
*	What lessons have been learned from any errors that were made? `r addRef(46)`
*	If any analyst involved were to leave the organisation or be unavailable, could the analytical process still be repeated? `r addRef(86)`

## Measuring and proving quality: advice for QA log books

Providing evidence of quality in statistics should be structured around the ESS quality dimensions outlined earlier in this document. One of the most common formats for a QA log is a checklist in the form of an Excel workbook. This is a useful format because it allows QA to be broken down by different stages and quality dimensions by creating different sheets within the workbook. The QA log can then be shared with analysts, commissioners, and assurers to keep all parties updated about the current state of QA activities and allow for assurer sign-off as needed. Such workbooks can also be reused and adapted for different projects, allowing a consistent QA approach. The question in this section represent examples of checklist elements that may be useful to include in a QA log. While they do not form a complete or comprehensive list of QA concerns, they are intended to provide a starting point for thinking about the assurance process.

### Relevance: Statistics meet the needs of users

Assessing user needs can be a complex and resource-intensive task but is important for ensuring the impact of statistics. A description of user needs may be drawn from direct consultation, usage statistics for past publications (such as web analytics), or user profiling based on an understanding of the intended audience of an output. This description can then help inform QA approaches. For instance, QA of analysis intended for the general public may be more concerned with guaranteeing accessible language and presentation than QA of analysis that will be used by researchers and specialists. Some examples of checklist elements that may be useful to include when assuring relevance are:

* Are we using the most recent and relevant user needs data?
* Have we met our priority user needs?
* Have user satisfaction concerns from previous publications been addressed, where applicable?

### Accuracy and Reliability: statistics accurately and reliably portray reality

This quality dimension requires the validation of not just end results of analysis, but also intermediate results and model inputs. While the analyst may conduct some checks here, it is preferable for other parties who are not responsible for analysis to be brought in for at least some checks as this increases the chance of catching errors. While automation is a very useful tool for the creation of models and reports, a small proportion of the automated components should be "spot-checked" to ensure that they are sensible and realistic.  Some examples of checklist elements that may be useful to include when assuring accuracy and reliability are:

*	Does running models with alternative input data yield expected behaviour?
*	Does running models with input data containing (realistic) extreme values yield expected behaviour?
*	Does running identical models using different software or platforms yield expected behaviour?
*	Do intermediate outputs from individual model or pipeline elements appear correct?
*	Has input data been quality assured?
* Have any missing data values been adequately addressed?
* How does the model output compare to other existing models?
* Does the output appear realistic for the real-world behaviour that is being modelled?
* Does the output appear realistic for the input data that was used?
* Do graphs, tables, and numerical results all appear to be in agreement with each other?
* Do the assumptions that were made remain sensible?
* Is the model actually performing the calculations defined in the model specification?
* Are automated components behaving as expected?


### Timeliness and Punctuality: statistics are released in a timely and punctual manner

Members of a team as well as end users of an output should be aware of its publishing schedule. Analysts should communicate and publicise when deviations from the schedule may be necessary at the earliest convenience. Some examples of checklist elements that may be useful to include when assuring timeliness and punctuality are:

* What is the expected release date and time?
* Has the publication been added to the release calendar?

### Coherence and Comparability: statistics are consistent internally, over time and comparable between regions and countries; it is possible to combine and make joint use of related data from different data sources

While methods, presentation formats, and questions of interest may change over time, it is important that these are not altered arbitrarily as this harms user ability to compare statistics. This extends not just to the editions of a recurring publication, but also all publications produced by a team or division. While it may not be feasible in all cases to apply identical practices to every output, teams should work toward defining common standards for their work and adhering to established conventions.  Some examples of checklist elements that may be useful to include when accessibility and clarity are:

* Do results seem sensible when compared to past results?
* Do results seem sensible when compared to results from other regions or studies?
* Have changes in methods been justified and explained?
* Has the same style guide been followed as previous publications?
* Have standard units, formats, and labels been used?
* Has version control software been used to document all changes?

### Accessibility and Clarity: statistics are presented in a clear and understandable form, released in a suitable and convenient manner, available and accessible on an impartial basis with supporting metadata and guidance

The need for accessibility and clarity applies to the contents of the publication  as well as to the publication itself. This means that sections of content should be easy to locate and identify in the document, and that the document itself should be easy to locate and identify on its host platform, taking into account the accessibility needs of a wide range of users. 

* Do spreadsheets of associated data adhere to accessibility guidelines?
* Has data been anonymised prior to publication?
* Is the methodology of the analysis clearly communicated to users?
* Is the quality of the analysis clearly communicated to users?
* Does content adhere to web accessibility guidelines?
* Is metadata for the publication correct and up to date?
* Is the level of technicality appropriate for the intended audience?

## References

`r AddReferenceList()`
