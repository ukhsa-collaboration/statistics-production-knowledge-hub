---
title: "Quality Assurance"
author: "Olivia Swarthout"
date: "5 December, 2022"    #"`r format(Sys.time(), '%d %B, %Y')`"
draft: true
output: 
  html_document:
    toc: true
    toc_depth: 3
    number_sections: false
    template: "- assets/Templates/html_template.html"
  word_document:
    reference_docx: "- assets/Templates/UKHSA plain document template.dotx"
knit: (function(inputFile, encoding){rmarkdown::render(inputFile, envir = globalenv(), encoding = encoding, output_dir = "./", output_format = "all")})

versioncontrol: "`r ukhsaGuidanceDocs::VersionHistory(  c('24/10/2022', 'First draft')  )`"
---


```{r setup, echo=F, include=F}
library(tidyverse)
library(readxl)
library(ukhsaGuidanceDocs)
knitr::opts_chunk$set(echo = F)
SetUpReferenceList("../../Team files/QPD/Knowledge bank/QPD knowledge bank.xlsx")
```


## Coverage

This guidance applies to everyone working in the production of statistics or other numerical analysis, whether for internal or external use.

## Summary of recommendations

1. Quality assurance should be considered at every step of analysis by everyone involved.
2. Quality assurance should ideally be planned in advance of statistical analysis, with roles, responsibilities, and the extent of assurance activities understood fully by the entire team. 
3. Quality analysis should be appropriate to the scope, risks, methodology, and data source of a piece of analysis. 
4. Analytical and quality assurance processes should be logged in detail as they are carried out. A designated assurer should confirm that a sufficient degree and standard of assurance has been conducted.
5. The quality assurance process and any outstanding quality concerns should be communicated alongside the final results of analysis in order to quantify confidence in the results.

## Introduction

### Defining quality

The quality of a statistical output may be thought of in simple terms as its "fitness for purpose" `r addRef(1,44,37,34)`. The ONS Code of Practice uses the 5 Dimensions of Quality of the European Statistical System (ESS) Code of Practice as criteria for assessing fitness for purpose of statistical outputs. These 5 dimensions are:

1. **Relevance:** statistics meet the needs of users
2. **Accuracy and Reliability:** statistics accurately and reliably portray reality
3. **Timeliness and Punctuality:** statistics are released in a timely and punctual manner
4. **Comparability and Coherence:** statistics are consistent internally, over time and 
comparable between regions and countries; it is possible to combine 
and make joint use of related data from different data sources
5. **Accessibility and Clarity:** statistics are presented in a clear and understandable 
form, released in a suitable and convenient manner, available and 
accessible on an impartial basis with supporting metadata and 
guidance

Quality assurance (QA) of a statistical output is the process of ensuring that these criteria have been met during its production and demonstrating this fact to users.

### Why we need quality assurance

The ONS Code of Practice includes quality as one of the 3 core pillars that support public confidence in statistics `r addRef(1)`. Principle Q3 of the Quality Pillar requires all producers of analytical outputs to explain how they have assured that their work is accurate, reliable, coherent and timely `r addRef(1)`. Similarly, a key goal of the Government Statistical Service (GSS) Quality Strategy is to ensure our outputs are of sufficient quality and communicate the quality implications to users `r addRef(84)`. 

This shows that there is a need to not only to guarantee quality but also demonstrate it to users, allowing them to use statistical publications with confidence in their validity `r addRef(84,85)`. High quality also helps build trust among those who are involved in policy and decision making `r addRef(44,37)`. Outputs that attain high levels of fitness-for-purpose allow better decisions to be made with greater efficiency.`r addRef(37, 84,32)`. The Goldacre Review explains the material benefits of high-quality analysis, noting that “these kinds of analyses deliver direct improvements in patient care by identifying problems early, and improving the efficiency of services for all.” `r addRef(84)` The risks posed by poor quality analysis are just as immediate and can include financial and legal damage as well as risks to health and livelihood `r addRef(34)`. It is therefore crucial that internal and external outputs are subject to rigorous quality assurance measures. 

## Roles and responsibilities

Goal 1 of the GSS Quality Strategy sets out the need for everyone involved in statistical production to know their role in assuring quality `r addRef(84)`. Team members should understand the exact QA plans for a project as well as their expected contributions `r addRef(85)`. 

The Aqua Book defines 3 key roles for QA of statistical analysis: the commissioner, the analyst, and the assurer `r addRef(34, 32)`. In practice, the exact nature of roles required may differ based on project scope and needs, but these 3 roles provide a useful framework for considering the different ways QA should be built into the life cycle of a project. 
 
The roles can be summarised as follows:

**The person commissioning analysis** (“the commissioner”) is responsible for:

* developing the question of interest and communicating this to the analyst and assurer so that they can identify the most appropriate methods for analysis and assurance
* clearly expressing the needs of the project and risks that are entailed so that proportionate levels of QA can be decided upon by the analyst

People of all levels of seniority commission analysis. The commissioner may be acting on behalf of a government decision-maker or other customer, or they may be a policy lead in the area where work is being commissioned. 

**The person leading the analysis** (“the analyst”) is responsible for: 

* working with the commissioner to help develop the question of interest in a way that allows it to be answered appropriately and meet the needs of the commissioner
* planning how they will demonstrate assured quality to the commissioner, assurer, and end users of their analytical output
* as needed, bringing in third party analysts to provide specialised advice and  quality assurance. 
* providing final results and records of all work they have done
* communicating to the commissioner the implications of their results, including strengths, limitations, uncertainty, and context

In practice, there may often be more than one analyst involved in a project: the use of the term “the analyst” is used for brevity’s sake.

**The person responsible for analytical assurance** (“the assurer”) is responsible for:

* ensuring that appropriate quality assurance is taking place throughout
* providing intermediate and final sign-offs for the output
* advising the commissioner on whether quality concerns have been sufficiently addressed and the implications of any remaining risks

This person is not directly involved in conducting QA and does not need to be an analyst. However, QA may benefit from an assurer who understands the analytical methods used in the project so that they can identify issues with the output. Because the assurer signs off on the QA activities, they should have enough seniority to take responsibility for the output. For example, they might be a senior analyst or analytical project manager.

More information about what each of these roles involves can be found in the next section. However, it is important to emphasise that QA does not begin and end with designated roles and project components. Quality can best be guaranteed by a culture of transparency and accountability that is supported by all members of a team. This includes encouraging everyone on the team to be open about their ideas, mistakes, and concerns without fear of repercussions or dismissive attitudes from senior staff. 

## How is Quality Assurance Conducted?

This section contains examples of questions that can guide QA throughout a project and suggested items of documentation that can be produced. However, the exact scale and scope of these QA activities should be considered on a case-by-case basis, ensuring that they are appropriate and proportionate to the needs of analysis `r addRef(34)`. More rigorous QA will demand more time and resources, and so the extent of QA should be justified by the level of risk present `r addRef(34,85)`. 

### Measuring and proving quality: advice for QA log books

At each stage of analysis, documentation should be produced detailing QA measures that have been undertaken. This helps guarantee that analysis and QA are transparent and reproducible, in line with ONS Code of Practice Q3.3 `r addRef(1)`, thereby helping to create high-quality statistics that retain value over time. 

Providing evidence of quality in statistics should be structured around the ESS quality dimensions outlined earlier in this document. One of the most common formats for a QA log is a checklist in the form of an Excel workbook. This is a useful format because it allows QA to be broken down into different stages and quality dimensions by creating different sheets within the workbook. The QA log can then be shared with analysts, commissioners, and assurers to keep them updated about QA progress and allow for assurer sign-off as needed. Such workbooks can also be reused and adapted for different projects. [There is an example spreadsheet available on our GitHub](...) containing examples of checklist elements that may be useful to include in a QA log. While it is not a complete list of QA concerns, the template is provides a starting point for thinking about the assurance process. Please refer to the guidance within the template for advice on how to use it. 

### Quality assurance responsibility throughout analysis

The responsibilities described below are examples of how leadership on tasks can be delegated between commissioner, analyst, and assurer. No single member of a team should be responsible for an entire task without any input or assurance from colleagues `r addRef(34)`. Though it may be one person’s job to lead and shape outputs for a specific aspect of analysis, all team members should have a thorough understanding of the principles behind the analysis and should be available to help as needed. 

#### 1. Commissioner engagement

This is usually the most preliminary stage of analysis and focuses on deciding the guiding question, scope, and context of the planned analysis. While there will be few outputs at this step to QA, the decisions made will define the level of QA necessary at later stages.

##### Suggested QA documentation:
* Specification or scoping document explaining the intended outcomes of the analysis, including risks and user needs, which may be edited in subsequent stages as details of the requested work change `r addRef(34)`. This can be used at the next stage to help create QA plans regarding relevance (ESS Quality Dimension 1) and user needs. 

##### Responsibilities:
* The **Commissioner** ensures that the intentions and complexities of the problem at hand are clearly communicated and that sufficient resources are secured for the analysis, including for appropriate quality assurance
* The	**Analyst** identifies the levels of quality, certainty, and precision that are required for the analysis. They should communicate with the commissioner to make sure that they both understand what is possible to deliver. This way, the analyst is able to complete the appropriate amount of QA that the project needs. (the analyst should also ensure they get the right information from the commissioner, who may not be familiar with this guidance or QA more generally)
*	The **Assurer** works with the analyst and commissioner to make sure that the proposed levels of quality and precision can be achieved. They may need to help the commissioner and analyst adjust their plans several times, because they should have a high degree of confidence in the project before they sign off on the scoping document 

##### Guiding questions:
* Have user needs been adequately considered? `r addRef(34)`
* Does the planned analysis align with user needs and project priorities? `r addRef(34)`
* Will the proposed time frames allow for adequate quality assurance? `r addRef(34)`
* Have areas of highest risk been identified and addressed? `r addRef(46)`
*	What is our data source? Is data quality-assured and appropriate for our purposes? `r addRef(32)`

#### 2. Planning analysis

This stage requires a higher degree of technical input as the commission details are converted into an analysis plan. While the analyst may have more responsibilites here, it is important for the commissioner to continually confirm that their specified needs will be met by the proposed methodology. 

##### Suggested QA documentation:
* QA plan that explains steps that will be taken at each step of the analysis to assure quality.`r addRef(34,46)`. This could include using our [QA log template](...) to create a blank QA log book with a row for each planned check. It may be useful to fill out the project timeline section of the template at this stage, too, to ensure that there is agreement and awareness across the entire team on plans for QA and analysis. This helps promote timeliness and punctuality (ESS Quality Dimension 3).

##### Responsibilities:
* The **Commissioner** maintains contact with the analyst and assurer to continually confirm that time frames will be sufficient for proposed levels of quality assurance
*	The **Analyst** should consider how they will ensure that their work will be robust, accurate, and adequate to address the research question, including considering the limits and uncertainty that these methods will entail and how these will be communicated in the final product. They should also draw up plans for QA of their work, communicating with the assurer to confirm the necessary methods and extent of assurance. How all aspects of the quality dimensions will be met should be carefully planned at this stage in order to ensure that analysis flows smoothly and meets the project needs 
* The	**Assurer** checks that the proposed level of QA for the analysis will be sufficient for the needs of the project

##### Guiding questions:
*	Is the planned analytical pipeline automated to reduce the risk of human error? `r addRef(46,35)`
*	Where automation is not possible, have risks of manual steps been addressed?  `r addRef(46)`
*	Is the planned process transparent and reproducible? `r addRef(32)`
*	Are the methods we are using best suited for the project? `r addRef(85)`
*	Have any potential quality issues in the data been considered? `r addRef(85)`

#### 3. Analysis

Analysis and QA of analysis become more efficient and transparent when they follow written plans specified in earlier stages. While some deviations from these plans are expected, they should be  thoroughly documented and communicated to the assurer and commissioner. A complete audit trail will allow for the sources of any issues that are identified during QA to be found more quickly. Ensuring a reproducible process is also a key part of aiding in peer reviews and audits after analysis has been completed.

It may aid efficiency to run QA activities parallel to analysis, passing outputs to assurers individually as they are produced. 

##### Suggested documentation:
*	QA log detailing automated and manual validation measures conducted during analysis to ensure results are accurate and reliable (ESS Quality Dimension 2) and coherent when compared to results from other time periods and regions (ESS Quality Dimension 4) `r addRef(34)`. This could involve using a QA log created from [our template](...) to fill out the section on verification and validation. 
*	Technical documentation of methods used, including:
    *	Documentation and commenting of code `r addRef(28)`
    *	Records of changes made, ideally managed via version control software such as Git `r addRef(28)`
*	Documentation and justification of assumptions `r addRef(34)`

##### Responsibilities:
* The **Commissioner** ensures that sufficient quality checking of analysis is being conducted for the needs of the project
* The	**Analyst** extracts and manages data, ensuring that data formats, units, and context are understood. They should record details, issues, and assumptions regarding the data and ensure that any instances of missing data are appropriately addressed and the consequences of missing data are recorded. As they follow their analytical plan, they should make note of any changes that they make to the plan as well as all QA measures that have been implemented. Ideally, they should call upon other analysts who are not involved in the project to conduct quality assurance. Issues that these assurers encounter should be flagged immediately and logged alongside measures taken to solve them. Where necessary, the analyst may also draw upon the skill of experts who can provide more specialised technical advice
*	The **Assurer** continually confirms that quality standards are being met and that the level of QA that has been carried out remains sufficient for the decision being supported

##### Guiding questions:
*	Are there any anomalies or unexpected trends in the data? Have they been sufficiently investigated? `r addRef(34,85)`
*	When changes have been made to analysis and QA methodologies, are they justified, well-documented, and clearly communicated? `r addRef(28)`
*	What assumptions does our analysis rely upon? Are these justified? `r addRef(86)`
*	Have the established best practice standards for code been followed? `r addRef(86,28)`

#### 4. Delivery of analysis

At this stage, it should be confirmed that the results of analysis are presented in a straightforward and transparent manner with all necessary context. The analysis may be approved in stages and may be sent back and forth between the commissioner, analyst, and assurer several times before it can be finalised.

##### Suggested documentation:
* QA log of checks conducted on documentation, accessibility, clarity, and other aspects of the analytical publication (ESS Quality Dimension 5). This could involve using a QA log created from [our template](...) to fill out the section on documentation and reproducibility as well as the section on accessibility and dissemination.
*	QA sign-off by assurer verifying that quality has been assured to the required standard `r addRef(86)`
*	Clarification of data and analysis quality, and the implications on results `r addRef(37)`
*	Quality and methods section accompanying the publication `r addRef(47)`

##### Responsibilities:
* The **Commissioner** passes results on to decision-makers and stakeholders, or sends the publication for upload to the relevant platform. Where the analysis has been commissioned specifically for policymaking, the commissioner should ensure that the details of results as well as limitations and outstanding risks are clearly communicated so that the analysis can be used appropriately
* The	**Analyst** ensures that results, risks, and limitations are clearly communicated to the commissioner and well-recorded in the final publication. They should ensure that any published materials have been quality-assured for accessibility and transparency. They are responsible for ensuring that the QA log has been completed and obtaining a final sign-off from the assurer. Looking back at the QA process and outcomes, they should reflect on the strong and weak aspects of the process, considering how they might apply what they have learned to future analysis
*	The **Assurer** checks that suitable QA has been completed and is well-documented. If so, the assurer signs off on the finished product. Otherwise, they should discuss concerns with the analyst and commissioner and ensure these are addressed before authorising delivery

##### Guiding questions:
*	Is the output accurate and error-free? `r addRef(46)`
* Without any input from analytical team, could a third party reproduce our results? `r addRef(86)`
*	If we repeat our processes with different software, will we get the same results? `r addRef(85)`
*	Have strengths and limitations been communicated clearly and effectively to users, stakeholders, and team members? `r addRef(1,35)`
*	Are data visualisations accessible and easy to understand? `r addRef(47)`
* Are sources cited and correctly linked? `r addRef(85,32)`
*	Have technical terms been defined, if necessary? `r addRef(47)`
*	What lessons have been learned from errors that were made? `r addRef(46)`
*	If any analyst involved were be unavailable, could the analysis still be repeated? `r addRef(86)`

### A note on ensuring data quality

Data may come from a large number of sources and may even be gathered by the analysts themselves. The complexities involved in gathering, storing, managing, and accessing data introduce many potential sources of error for a piece of analysis - before analysis even commences! 

Ensuring high data quality at the point of collection is an extremely involved task that depends on a huge number of situational factors and is therefore beyond the scope of this guidance. However, the validation of subsequent stages of the "data lifecycle" bears discussing here as it is a key aspect of ensuring high quality statistical outputs. Under Q3 of the Quality Pillar, the ONS Code of Practice explains that "Statistics should be based on the most appropriate data to meet intended uses. The impact of any data limitations for use should be assessed, minimised and explained." `r addRef(1)` 

When obtaining data from a database or other storage platform, it is best for the analysts  to access the data themselves rather than request it from a provider. However, this may not be possible in some circumstances, and data may have to be requested from a provider. In this case, the data providers should keep complete records of the queries used to obtain the requested data and share those with analysts `r addRef(86,1)`. A copy of the original spreadsheets provided should also be saved before data cleaning begins, allowing for the data cleaning process to be documented from beginning to end and for changes to be reversed if needed. Our [guidance on reproducible analytical pipelines (RAP)](...) notes that software such as SQL can be used to query databases using code, which provides a record of how data was obtained.

Some manual checking is usually necessary for analysts to identify unreasonable values or distributions `r addRef(86)`, but automated checks can be built into the data cleaning pipeline too. Tools such as RMarkdown notebooks can be used to create detailed summaries of data and flag any potential issues such as outliers or missing values. Any bias, uncertainty, or missing values in the data should be documented by the analyst. In the final publication, the analyst should explain the implications these have on the outputs. `r addRef(34)`

## References

`r AddReferenceList()`
