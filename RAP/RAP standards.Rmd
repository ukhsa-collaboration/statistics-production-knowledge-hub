---
title: "Reproducible Analytical Pipelines (RAP): Bronze, Silver and Gold standards"
author: "Ben Cuff"
date: "10 November 2022"    #"`r format(Sys.time(), '%d %B, %Y')`"
draft: true
output: 
  html_document:
    toc: true
    toc_depth: 3
    number_sections: false
    fig_caption: no
    template: "../- assets/Templates/html_template.html"
  word_document:
    fig_caption: no
    reference_docx: "../- assets/Templates/UKHSA plain document template.dotx"
knit: (function(inputFile, encoding){rmarkdown::render(inputFile, envir = globalenv(), encoding = encoding, output_dir = "./", output_format = "all")})

versioncontrol: "`r ukhsaGuidanceDocs::VersionHistory( c('05/12/2022', 'Added overview table'), c('10/11/2022', 'First release')  )`"
---
  
  
```{r setup, echo=F, include=F}
library(tidyverse)
library(ukhsaGuidanceDocs)
knitr::opts_chunk$set(echo = F)

SetUpReferenceList("../../Team files/QPD/Knowledge bank/QPD knowledge bank.xlsx")

```

## Standards of RAP

The purpose of this document is to outline a number of standards to work towards when developing and improving upon a Reproducible Analytical Pipeline (RAP). For an introduction on what RAP is, and what benefits it can bring, please see our other guidance: "[Introduction to Reproducible Analytical Pipelines (RAP)](https://htmlpreview.github.io/?https://github.com/ukhsa-collaboration/statistics-production-knowledge-hub/blob/main/RAP/RAP.html)".

In this document, we present bronze, silver and gold standards, offering major benchmarks to work towards, with each subsequent standard reflecting a more transparent, reproducible and robust pipeline. These are largely based upon the "Minimum" and "further" standards of RAP developed by the Government Analysis Function `r addRef(18)`. For an explanation of how our standards differ from theirs, please see [the appendix](#appendix). 

While we would like all analytical pipelines to eventually meet one of these standards, it is important to stress that RAP does not need to be an all or nothing exercise. Even implementing just some of the principles outlined here will bring about improvements to your processes and outputs `r addRef(41, 31)`. Trying to achieve all of these standards in one go may be too daunting a task and so incremental improvements are the suggested way to go.

It is also important to recognise that some of principles that form these standards may not always be possible. For example, where remote connections to databases are not permitted, this cannot be automated (which is one of the principles in our bronze standard). In such cases, the aim should be to apply RAP to the other areas where principles can be applied `r addRef(23)`. (Note though that it is preferred to have a direct connection to databases wherever possible, as any "black-box" processes conducted in extracting and preparing data for analysis might undermine some of the benefits of RAP `r addRef(35)`.)

We will be implementing a "RAP register" alongside our guidance to map different our analytical pipelines against these standards, to measure progress, to map knowledge (so that it can be shared!), and to celebrate our successes! The RAP register is available to internal UKSHA colleagues only. Those colleagues can contact [UKHSA.RAP@ukhsa.gov.uk](mailto:UKHSA.RAP@ukhsa.gov.uk) for a link. For non-UKHSA people, [the example template can be found on our GitHub repository](https://github.com/ukhsa-collaboration/statistics-production-knowledge-hub/blob/main/RAP/RAP%20register%20template.ods).`

For more support on how to meet these standards, including information on how to join our helpful UKHSA RAP community, please contact [UKHSA.RAP@ukhsa.gov.uk](mailto:UKHSA.RAP@ukhsa.gov.uk).


### Overview

The following table gives a quick overview of the principles that form each standard. More detailed explanations are given in the sections to follow. 


| The Bronze standard | The Silver standard | The Gold standard |
|----|----|----|
| Have minimal manual steps for data extraction and analysis | Meet the Bronze standard | Meet the Silver standard |
| Use open source analytical software | Have minimal manual steps for the production of outputs | Have unit testing |
| Integrate quality assurance checks throughout the analysis | Use functions as reusable blocks of code | Have error handling for functions |
| Have well-commented code and documentation | Adhere to a common code style | Include documentation for functions |
| Make code available to other analysts | Have automated input data validation | Use packaging |
| Use peer review to ensure reproducibility | Use version control software | Log data and analysis checks |
| - | - | Implement continuous integration |
| - | - | Implement dependency management |



### The Bronze standard

The checkboxes in the following lists are there for you to use when assessing your pipeline.

To meet the bronze standard for RAP, your project should :

* <input type="checkbox"> have minimal manual steps (for example, minimal copy-paste, point-click, drag-drop operations) for data extraction where permissions allow (for example, using SQL code), and for the analysis steps used to produce numbers, tables and charts 

* <input type="checkbox"> use open source analytical software (preferably R or Python)

* <input type="checkbox"> integrate quality assurance checks throughout the analysis, automated where appropriate, supplemented with semi-automated and manual checks (we will be producing further guidance on this in due course on [the knowledge hub](https://github.com/ukhsa-collaboration/statistics-production-knowledge-hub/blob/main/README.md)) 

* <input type="checkbox"> have well-commented code and documentation embedded as part of the project, rather than being saved elsewhere

* <input type="checkbox"> be open and available to other analysts (including external users where appropriate)

* <input type="checkbox"> use peer review to ensure that the analysis is accurate and reproducible, and that the pipeline meets the rest of this standard


Minimising the number of manual steps helps reduce the risk of human error. While automation is encouraged, this does not mean the entire pipeline needs to be run in one step. For example, trying to pull and shape data and produce outputs in the same line(s) of code might make it difficult for other analysts to follow what you are doing or reuse parts of your code for similar projects `r addRef(50)`. Instead, your project might be separated out into several scripts denoting different stages of the pipeline. For example, you might have one script that pulls the data from the data store and condenses it down into one or more minimum tidy datasets: defined as the minimum amount of data needed to complete the publication `r addRef(50)`. Those datasets can then be passed through an RMarkdown script to produce the report (or easily picked up for use in different projects!). 

Modern tools such as R or Python are preferred over legacy tools such as SPSS, Stata or SAS because they are better at reading a range of different data formats, support modularised code, and can be used to automate the creation outputs in a range of different formats (for example, markdown, HTML, Word, Excel and PowerPoint files - this will become important for meeting the silver standard principles later on) `r addRef(27)`. They also reduce (or eliminate) the number of times where data needs to be moved from one programme into another (for example, from SPSS into Excel into Word) `r addRef(40)`.

Simply being reproducible does not mean that you are doing your analysis correctly `r addRef(39)`. It is important to quality assure each stage of the process. Checks should be built in at various points throughout the code to flag anything unexpected for further (human) exploration. These might print warning messages into the console for example, or output more detailed markdown files flagging where potential issues might exist. Note that while many quality assurance checks can be automated, some human input will always be needed, such as for final proof readings and checking that automated text still matches the figures `r addRef(50)`. Quality assurance should align with the principles outlined in the Aqua Book `r addRef(34)` and the Code of Practice for Statistics `r addRef(1)` for those who have adopted the code. We are currently producing further guidance on quality assurance, which will be available on [the knowledge hub](https://github.com/ukhsa-collaboration/statistics-production-knowledge-hub/blob/main/README.md) soon. 

Having good quality documentation accompanying your code is important for reproducibility. The simplest form of documentation is code comments. These can be supplemented with additional documentation such as a "README" file `r addRef(35, 27)`. Good documentation will help improve the reuse of code, either by other analysts using it for the first time or by the same analyst using it in the future `r addRef(35)`. It should explain what the code does and how it can be used, including what options or parameters are available, examples of how to run it, and an explanation of any software or environment dependencies `r addRef(39)`.

It is a good idea to make code available to others because this means that the pipeline can be reproduced should the usual analyst go on leave or leave the organisation. It should be stored in a place that is accessible to others (not on a local drive). Keeping code in a repository such as GitHub can also make it easier to find and provides a back up in case of accidental deletion `r addRef(39)` (excluding anything sensitive such as datasets and database connection details). Ideally, the code should be made publicly available for transparency. However, this may not always be appropriate `r addRef(40)`. Please adhere to our organisational policy on this (which is currently being developed).

The purpose of the peer review here is to ensure that the analysis is fully reproducible by running it on a different machine and getting the same results without input from the original author. The review should also ensure the pipeline appropriately follows the other principles outlined by this standard (and the silver and gold standards, should the pipeline also meet the principles outlined there). 


### The Silver standard

To meet the silver standard for RAP, your project should:

* <input type="checkbox"> achieve all of the points covered under the bronze standard

* <input type="checkbox"> have minimal manual steps (for example, minimal copy-paste, point-click, drag-drop operations) for the production of spreadsheet workbooks and reports for publication 

* <input type="checkbox"> use functions as reusable blocks of code

* <input type="checkbox"> adhere to a common best practice code style

* <input type="checkbox"> have automated input data validation

* <input type="checkbox"> use version control software such as Git and GitHub to create and maintain a recorded history of the project




Automating the production of outputs further reduces the risk of human error. There are packages available to help produce reports automatically, such as [RMarkdown](https://rmarkdown.rstudio.com/) and [Python-Markdown](https://pypi.org/project/Markdown/). Spreadsheet workbooks can similarly be produced using the [openxlsx package for R](https://cran.r-project.org/web/packages/openxlsx/index.html), for example. Producing outputs in these ways means that you know all elements of the output are in sync, as everything is produced from that single pipeline `r addRef(40)`. With more manual approaches, forgetting to update the charts, for example, will mean that they are no longer in sync with what is presented in the text. A good test for how automated your pipeline is might be to think whether or not your outputs are deletable without worry. If you would not want to delete your outputs, because it would take time and energy to re-make them, then your end-to-end pipeline is not fully automated `r addRef(28)`.  

Functions are particularly useful when you are repeating the same operations at multiple points in your code. Rather than writing out the same code each time you need to carry out an operation, you can create a function and call that each time instead. This lessens the risk of error because if you need to change that operation, you only need to change the function definition, not each place in the code where it is run (some of which may otherwise be missed). Functions also lend themselves nicely to sharing snippets of functional code with others `r addRef(35)`. 

Working to a common code style makes it easier for other analysts to pick up your code as they will be able to interpret and understand it more quickly. We will be looking to develop a code style guide in the future. For R users, [the tidyverse style guide](https://style.tidyverse.org/) is the most commonly used style in the global community. 

Validating your input data is important as the quality of your outputs will necessarily depend on the quality of the inputs. Remember: ["garbage in, garbage out"](https://dictionary.apa.org/garbage-in-garbage-out). Validating those inputs in an automated way at the very start of the pipeline allows you to quickly identify and address any quality issues early. You might produce code to check, for example, whether there have been any unexpectedly large changes in the latest iteration of a longitudinal dataset. Other sense checks can also be performed. You could have your code produce a markdown document which flags any rows that may warrant further exploration - this may be particularly useful for any reports which are run on a regular basis.

Version control is important for documenting what changes have been made when, why, and by whom, as well as helping to ensure that people running your code know that they are using the right version `r addRef(28, 40, 31, 27, 39)`. It can act as a safety net, allowing you to roll back to earlier versions of code in the event of an issue, or to review how an earlier report was produced `r addRef(35, 39)`. 

[GitHub](https://github.com/) (or [GitLab](https://about.gitlab.com/)) can be used as a central repository to store your code and version history, allowing it to be easily accessed by others. Sharing code via GitHub can also help avoid duplication of work when other analysts are looking to perform similar tasks `r addRef(18)`. GitHub also allows analysts to be open about areas they have identified for improvement via an issues log, also inviting ideas from others `r addRef(31)`. Private repositories can be used for internal use only, or public repositories can be used to make your code more openly available. Please adhere to our organisational policy on open repositories (which is currently being developed).

It is important not to commit anything sensitive to version control repositories, such as datasets, secret keys and credentials. Datasets should be excluded to avoid disclosing any personal information. It is recommended that keys and credentials should be saved in a separate file that can still be used within the project, but which is excluded from the repository. Pre-commit hooks, such as those provided by the "govcookiecutter" tool `r addRef(30)`, or ".gitignore" exclusions can help mitigate this risk. Remember that even when sensitive information is included deep within the version history, these can be checked out and viewed `r addRef(28)`.  

While datasets are usually not committed to repositories, it is still important to ensure that some form of version control is applied to the source data, so that others know what version of the data was used for the analysis `r addRef(27)`. If your project relies on snapshot CSV files, rather than connecting to a database directly, it is good idea to store those files in a read-only folder, to prevent them being modified or deleted - any changes like this could make it difficult to reproduce the same outputs in the future `r addRef(28, 27, 39)`. For these files, it is important to ensure that they are saved prior to any data cleaning taking place, so that that data cleaning process can be documented within the production pipeline `r addRef(28)`. 

Even where direct connections to databases are made, it may be a good idea to save a copy of a tidy dataset (the minimum dataset needed to produce an output, rather than the full underlying dataset `r addRef(40,41)`) at some point in the pipeline to keep a record of the data used, and so that it can also be reused in other projects `r addRef(41)`. Please adhere to proper information governance procedures when doing this, however, avoiding saving copies of anything that might be considered sensitive. 


### The Gold standard

To meet the gold standard for RAP, your project should:

* <input type="checkbox"> achieve all of the points covered under the bronze and silver standards

* <input type="checkbox"> have unit testing for functions
    
* <input type="checkbox"> have error handling for functions

* <input type="checkbox"> include documentation of functions (usually included as part of a package)

* <input type="checkbox"> use packaging

* <input type="checkbox"> log data and analysis checks 

* <input type="checkbox"> implement continuous integration

* <input type="checkbox"> implement dependency management


Unit testing is where functions are tested with controlled inputs each time they are called, to check that expected outputs are returned. This can be a useful method to raise alarms if changes made to code elsewhere in the project, or some other environment change, adversely affects the function's operation `r addRef(41, 35, 40)`. The principle behind unit testing is that where the functionality of the smallest units of the code can be guaranteed, it is more likely that the overall project is running as expected `r addRef(41)`. They also encourage analysts to focus more on quality assurance, and what the purpose of each piece of code is `r addRef(41)`. Unit tests further act as another layer of documentation: it tells users what the intention behind a certain piece of code was and what the expected outputs are `r addRef(41)`.

[Error handling for functions](https://www.r-bloggers.com/2012/10/error-handling-in-r/) involves stating how the function should behave when it encounters something unexpected, such as a missing or incorrectly formatted parameter passed through the function call. Common actions might include stopping the code with an error message, or printing a warning message but allowing the code to continue. 

It is sensible to provide documentation for your functions so that others know how to use them. Aspects of functions to document include its purpose, the parameters required in function calls along with any default values, and expected inputs and outputs. The preferred place to include this documentation would be within package help files and vignettes.

A package more formally captures all the code and documentation required for a project or collection of functions in one place `r addRef(40)`. It also allows for easier sharing of all the functions needed for a pipeline, or indeed the whole pipeline itself, as well as enhanced version control for the project (via package version numbers). 

Producing logs of data and analysis checks can help analysts, users and quality assurers to understand whether anything is going wrong with the pipeline, and to understand how and where to respond to any issues. These logs might include, for example, the outputs of any input data validation, unit tests, other checks, messages and warnings, as well as more manual checks. Please see our [guidance on quality assurance](https://github.com/ukhsa-collaboration/statistics-production-knowledge-hub/blob/main/README.md) (coming soon) for more information. The Office for National Statistics are also planning to produce a chapter on this in the Duck Book `r addRef(28)`. 

Continuous Integration (CI) describes the managed process of accepting and integrating changes made by individual analysts into a definitive main version of a project. For example, it includes the management of conflicts and the running of regular (automated) tests to ensure that no bugs have been introduced by recent changes `r addRef(39)`. CI helps project leads to identify and resolve any conflicts and bugs early, and helps keep individual collaborators up to date with developments `r addRef(39)`. [Pull requests](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests) can be used in GitHub as part of this purpose, to require a sign-off step before any new code is accepted into the main pipeline `r addRef(40)`. Branches can also be used to work on changes without the affecting the main pipeline until your are ready, for example to build in a new feature or when fixing a bug `r addRef(28, 39)`.

Dependency management is important as your code is ultimately dependent on the specific state of the software used at the time the analysis is run `r addRef(35, 40)`. If the environment differs for another analyst, or for the same analyst in the future (for example, following software and package updates), the outputs may differ. A simple illustration of this kind of issue is provided by the Turing Way Community `r addRef(39)`: A basic division of 1 by 5 can either return 0 or 0.2, depending on which version of Python is being used (Python 2 defaults to integer division; Python 3 does not). To combat risks such as these, details of the environment can be stored, such as via the [renv package for R](https://cran.r-project.org/web/packages/renv/index.html) or via [a Docker image](https://www.docker.com/). These tools save a record of the environment in which the analysis was originally performed, allowing others to restore that same environment.


## References

`r AddReferenceList()`


## Appendix: our standards versus those produced by the Government Analysis Function {#appendix}

Our guidance draws upon the principles developed by the Analysis Function `r addRef(18)`, supported by other sources given in the reference section. However, we have separated them out into 3 standards (bronze, silver and gold) instead of the Analysis Function's 2 (minimum and further). This is to make it easier for teams to progress through the standards in smaller leaps. 

In our silver standard, some principles have come from the Analysis Function's minimum standard, and others from their further standards. Specifically:
 
* "minimise manual steps" has been separated out into two principles covering manual steps during analysis and manual steps during the production of spreadsheets and workbooks, with the former remaining in the minimum bronze standard and the latter being moved to our silver standard - this is to recognise the fact that many analysts will develop the more basic programming skills needed for analysis first, before developing other skills around Rmarkdown, for example
* "using version control software" has also been moved from the minimum standard to our silver standard for similar reasons (learning Git tends to come after learning R or Python)
* "validating input data", "using functions", and "adhere to a common best practice code style" have all been moved to our silver standard from the further standard because we have found that these are usually easier to implement than the other further standard principles.

Aside from these differences, all other Analysis function "minimum" standard principles are in our bronze standard, and all other Analysis Function "further" standard principles are in our gold standard.

<br>